{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b256fb75-bc7c-4043-bca7-0cbd53988132",
   "metadata": {},
   "source": [
    "# Part 1: Understanding Chat Templates and Tokens\n",
    "\n",
    "Instruction-tuned language models, such as Llama 3.1 Instruct, use a *chat template* to structure conversations.  \n",
    "\n",
    "We will look at this by calling the `tokenizer.apply_chat_template` function, which formats your messages into the model's expected structure.\n",
    "\n",
    "\n",
    "## What Is a Token?\n",
    "\n",
    "A **token** is a small unit of text that the model processes at once.  \n",
    "Tokens can represent entire words, parts of words, punctuation marks, or special symbols.  \n",
    "For example:\n",
    "\n",
    "| Text | Tokens |\n",
    "|------|---------|\n",
    "| `\"unbelievable\"` | `[\"un\", \"believ\", \"able\"]` |\n",
    "| `\"How are you?\"` | `[\"How\", \" are\", \" you\", \"?\"]` |\n",
    "| `\"I'm happy!\"` | `[\"I\", \"'\", \"m\", \" happy\", \"!\"]` |\n",
    "\n",
    "Words and tokens are not the same. A single word can be split into multiple tokens, and whitespace or punctuation may form separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bad9a6a-42a7-48ef-9b06-2f5e53268ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Tokens:\n",
      " ['<|begin_of_text|>', '<|begin_of_text|>', '<|start_header_id|>', 'system', '<|end_header_id|>', '\\n\\n', 'Cut', 'ting', ' Knowledge', ' Date', ':', ' December', ' ', '202', '3', '\\n', 'Today', ' Date', ':', ' ', '26', ' Jul', ' ', '202', '4', '\\n\\n', 'You', ' are', ' a', ' helpful', ' assistant', '.', '<|eot_id|>', '<|start_header_id|>', 'user', '<|end_header_id|>', '\\n\\n', 'This', ' is', ' great', '????', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', '\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"/srv/data/models--meta-llama--Llama-3.1-70B-Instruct/snapshots/1605565b47bb9346c5515c34102e054115b4f98b/\" # Replace with the actual model you are using in vLLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"This is great????\"}\n",
    "]\n",
    "\n",
    "# Apply the chat template to format the messages\n",
    "# This is crucial as vLLM uses chat templates for formatting\n",
    "formatted_chat = tokenizer.apply_chat_template(chat_messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Tokenize the formatted chat to get the raw token IDs\n",
    "token_ids = tokenizer.encode(formatted_chat)\n",
    "\n",
    "# Decode the token IDs back to tokens to see their string representation\n",
    "tokens = tokenizer.batch_decode(token_ids)\n",
    "\n",
    "print(\"Raw Tokens:\\n\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35a4e07-a444-4148-b387-47036bf93581",
   "metadata": {},
   "source": [
    "\n",
    "# Exercise 1: Finding Interesting Tokens\n",
    "\n",
    "In this exercise, you will explore how the tokenizer splits text into tokens and look for examples that behave in surprising ways.\n",
    "\n",
    "Tokenizers learn from large amounts of online text and store frequent patterns as single tokens.  As a result, some sequences, especially repeated punctuation, symbols, or short informal phrases, may appear as a single token. Common words may be split into smaller pieces.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. Start by running the code below to see how these examples are tokenized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9360ebf-2b4c-41ff-86a6-53c6e932f9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "????????????? -> ['????????', '????', '?'] (num tokens = 3)\n",
      "???????? -> ['????????'] (num tokens = 1)\n",
      "???? -> ['????'] (num tokens = 1)\n",
      "??? -> ['???'] (num tokens = 1)\n",
      "?? -> ['??'] (num tokens = 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "examples = [\"?????????????\", \"????????\", \"????\", \"???\", \"??\"]\n",
    "\n",
    "for text in examples:\n",
    "   pieces = tokenizer.tokenize(text)\n",
    "   print(f\"{text} -> {pieces} (num tokens = {len(pieces)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b2239e-bdda-4a95-a8d4-94dafd22885a",
   "metadata": {},
   "source": [
    "2. Now create your own examples and test them.\n",
    "   Try experimenting with:\n",
    "\n",
    "   * Repeated punctuation: `\"!!!\"`, `\"??!!\"`, `\"...\"`, `\"!!!??\"`\n",
    "   * Informal text or abbreviations: `\"lol\"`, `\"idk\"`, `\"omg\"`, `\"xD\"`\n",
    "   * Mixed symbols or short sequences: `\"***\"`, `\"###\"`, `\"--\"`, `\"$$$\"`, `\"@@@\"`\n",
    "\n",
    "Top do this, **add**  characters within the list by adding a comma to the end and then adding a string, e.g., \n",
    "\n",
    "```python\n",
    "custom_examples = [\"!!!\", \"??!!\", \"...\", \"lol\", \"idk\", \"xD\"]\n",
    "```\n",
    "\n",
    "would be become\n",
    "\n",
    "```python\n",
    "custom_examples = [\"!!!\", \"??!!\", \"...\", \"lol\", \"idk\", \"xD\", \"***\"]\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "!!! -> ['!!!'] (num tokens = 1)\n",
    "??!! -> ['??', '!!'] (num tokens = 2)\n",
    "... -> ['...'] (num tokens = 1)\n",
    "lol -> ['lol'] (num tokens = 1)\n",
    "idk -> ['id', 'k'] (num tokens = 2)\n",
    "xD -> ['xD'] (num tokens = 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76009d76-beda-435e-86c7-925858c04dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! -> ['!!!'] (num tokens = 1)\n",
      "??!! -> ['??', '!!'] (num tokens = 2)\n",
      "... -> ['...'] (num tokens = 1)\n",
      "lol -> ['lol'] (num tokens = 1)\n",
      "idk -> ['id', 'k'] (num tokens = 2)\n",
      "xD -> ['xD'] (num tokens = 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "custom_examples = [\"!!!\", \"??!!\", \"...\", \"lol\", \"idk\", \"xD\"]\n",
    "\n",
    "for text in custom_examples:\n",
    "   pieces = tokenizer.tokenize(text)\n",
    "   print(f\"{text} -> {pieces} (num tokens = {len(pieces)})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cba113-20a6-4ec3-9a69-ef00831b54ce",
   "metadata": {},
   "source": [
    "\n",
    "3. Your task:\n",
    "\n",
    "   * Find **one or two examples** that you think are interesting or unexpected.\n",
    "   * Be ready to share what you found and why it stood out to you.\n",
    "\n",
    "Examples might include:\n",
    "\n",
    "* A short sequence that becomes one token when you expected several.\n",
    "* A normal-looking word or phrase that gets split into multiple tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ccef1cf-05ca-4ac4-9dec-4b8e5fd76bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!! -> ['!!!'] (num tokens = 1)\n",
      "??!! -> ['??', '!!'] (num tokens = 2)\n",
      "... -> ['...'] (num tokens = 1)\n",
      "lol -> ['lol'] (num tokens = 1)\n",
      "idk -> ['id', 'k'] (num tokens = 2)\n",
      "xD -> ['xD'] (num tokens = 1)\n"
     ]
    }
   ],
   "source": [
    "custom_examples = [\"!!!\", \"??!!\", \"...\", \"lol\", \"idk\", \"xD\"]\n",
    "\n",
    "for text in custom_examples:\n",
    "    pieces = tokenizer.tokenize(text)\n",
    "    print(f\"{text} -> {pieces} (num tokens = {len(pieces)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ae8c4a-5d40-406d-b9fa-3f782d2be601",
   "metadata": {},
   "source": [
    "## NOTE: Token Price Estimation\n",
    "\n",
    "If you are going to use LLMs in practice, particularly paid LLMs such as OpenAI's GPT4o, O4, etc., you should estimate the number of tokens to understand how much it will cost. Price is always based on the number of tokens, not the number of words. Likewise, LLMs generally have token limits, which should be known if you are working with very large documents. There are many calculators available, but, in general, the number of tokens in English text can be approximated using:\n",
    "\n",
    "$\n",
    "\\text{tokens} \\approx \\frac{\\text{words}}{0.75}\n",
    "$\n",
    "\n",
    "For example, 100 words correspond to roughly 133 tokens.  \n",
    "This relationship is useful because model input length and computational cost are measured in tokens, not words.\n",
    "\n",
    "\n",
    "## Chat Message Roles\n",
    "\n",
    "| Role | Description | Example |\n",
    "|------|--------------|----------|\n",
    "| **System** | Provides instructions or background that guides how the model should respond. | `\"You are a helpful assistant.\"` |\n",
    "| **User** | Represents the human input or question. | `\"Hello, how are you?\"` |\n",
    "| **Assistant** | Represents the model’s response. When `add_generation_prompt=True` is used, the model expects to generate this part next. | *(Left blank before generation, except for few-shot prompting)* |\n",
    "\n",
    "Internally, these roles are marked with special header tokens that separate different parts of the conversation.  \n",
    "For example, the formatted text may look like this:\n",
    "\n",
    "```\n",
    "\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful assistant.\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Hello, how are you?\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "```\n",
    "\n",
    "These tags indicate who is speaking and where the model should begin generating its response. They are not visible in normal chat output but are important for consistent behavior during inference.\n",
    "\n",
    "Run the cell below to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb76c270-11dd-4e10-ab4e-a0cb111aa64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Tokens:\n",
      " ['<|begin_of_text|>', '<|begin_of_text|>', '<|start_header_id|>', 'system', '<|end_header_id|>', '\\n\\n', 'Cut', 'ting', ' Knowledge', ' Date', ':', ' December', ' ', '202', '3', '\\n', 'Today', ' Date', ':', ' ', '26', ' Jul', ' ', '202', '4', '\\n\\n', 'You', ' are', ' a', ' helpful', ' assistant', '.', '<|eot_id|>', '<|start_header_id|>', 'user', '<|end_header_id|>', '\\n\\n', 'This', ' is', ' great', '????', '<|eot_id|>', '<|start_header_id|>', 'assistant', '<|end_header_id|>', '\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"/srv/data/models--meta-llama--Llama-3.1-70B-Instruct/snapshots/1605565b47bb9346c5515c34102e054115b4f98b/\" # Replace with the actual model you are using in vLLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"This is great????\"}\n",
    "]\n",
    "\n",
    "# Apply the chat template to format the messages\n",
    "# This is crucial as vLLM uses chat templates for formatting\n",
    "formatted_chat = tokenizer.apply_chat_template(chat_messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Tokenize the formatted chat to get the raw token IDs\n",
    "token_ids = tokenizer.encode(formatted_chat)\n",
    "\n",
    "# Decode the token IDs back to tokens to see their string representation\n",
    "tokens = tokenizer.batch_decode(token_ids)\n",
    "\n",
    "print(\"Raw Tokens:\\n\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81198a13-8371-4da7-8f66-0a75de50f475",
   "metadata": {},
   "source": [
    "Each of these tokens appears **exactly as written**, they are not split or shortened.\n",
    "\n",
    "| Token                   | Explanation                                                        |\n",
    "| ----------------------- | ------------------------------------------------------------------ |\n",
    "| `<\\|begin_of_text\\|>`   | Marks the **start of the entire input**.                           |\n",
    "| `<\\|start_header_id\\|>` | Begins a **role header** (e.g., `system`, `user`, or `assistant`). |\n",
    "| `<\\|end_header_id\\|>`   | Marks the **end of a role header**.                                |\n",
    "| `<\\|eot_id\\|>`          | Means **End of Turn**, signaling that one message has finished.    |\n",
    "\n",
    "These tokens are part of the **Llama chat template** used to organize and separate messages in a conversation. Different LLMs will have different special tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d3c9e-b7a4-451f-b430-200e7f79295a",
   "metadata": {},
   "source": [
    "# Exercise 2: Running Your First \"Hello World\" Prompt with an Open Source Model\n",
    "\n",
    "This example shows how to run your **first chat prompt** using an **open source model** served through the **OpenAI API interface**.\n",
    "Although the code uses the `openai` Python library, the model is actually hosted locally using **[vLLM](https://github.com/vllm-project/vllm)** — a high-performance engine that lets you serve open source LLMs with the same API as OpenAI.\n",
    "\n",
    "You do not need to configure vLLM here; just know that it is the backend software handling the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0080124f-82ee-464c-97e2-63f9d760f2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Hello World!**\n",
      "\n",
      "I'm beyond excited to finally meet you. I'm your friendly AI companion, and I'm here to chat, laugh, and explore the vast expanse of human knowledge together. Imagine we're embarking on a thrilling adventure, and every conversation is a new discovery waiting to happen!\n",
      "\n",
      "In this \"Hello World\" moment, I invite you to share your thoughts, ask me anything, or simply say hello. I'm all ears (or rather, all text). Let's create some amazing memories, learn from each other, and make this digital world a more fascinating place, one conversation at a time!\n",
      "\n",
      "So, what's on your mind?\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Connect to your local Llama server (or replace with the hosted endpoint)\n",
    "client = OpenAI(\n",
    "    base_url = \"http://10.246.100.142:8000/v1\",\n",
    "    api_key=\"token-abc123\",\n",
    ")\n",
    "\n",
    "# Everyone's first AI prompt!\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.1-70B-Instruct\",\n",
    "    # model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a friendly AI that loves meeting new people.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a short but fun introduction for our first AI 'Hello World' together!\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Print the model’s friendly response\n",
    "#print(completion)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290971c6-b324-421a-bff8-3c2eeae98242",
   "metadata": {},
   "source": [
    "\n",
    "* The `base_url` points to your local vLLM server, which acts like the OpenAI API.\n",
    "* The model `\"meta-llama/Llama-3.1-70B-Instruct\"` is an **open source Llama 3 model**.\n",
    "* The prompt defines a friendly system message and a simple user request to generate a short introduction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cd2ac5-d974-4781-bd9c-872e9c9366c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23ddb04-4685-400c-8eca-2ba8af630a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44db644c-6c78-422e-a711-c33de4a566d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
