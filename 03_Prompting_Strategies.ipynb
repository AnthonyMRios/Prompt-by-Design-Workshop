{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89568135-5be2-4aa0-b8d7-6122681c3a49",
   "metadata": {},
   "source": [
    "# Part 3: Zero-Shot, Few-Shot, and Chain-of-Thought Prompting\n",
    "\n",
    "Large language models (LLMs) can often perform tasks directly from instructions. This ability to follow a new instruction without additional examples is known as **zero-shot prompting**.  \n",
    "However, as tasks become more nuanced or reasoning-intensive, models benefit from additional context or demonstrations provided directly in the prompt.  \n",
    "This leads to more advanced techniques: **few-shot prompting** and **chain-of-thought prompting**.\n",
    "\n",
    "\n",
    "## 1. Zero-Shot Prompting\n",
    "\n",
    "Zero-shot prompting relies purely on the model’s pretraining knowledge and the clarity of your instruction.  \n",
    "You describe what you want, and the model attempts to infer the task without examples.\n",
    "\n",
    "### Example: Classifying a San Antonio Community Message\n",
    "\n",
    "**Prompt:**\n",
    "```\n",
    "\n",
    "Classify the following message as one of: Public Works, Community Events, or Public Safety.\n",
    "\n",
    "Message: There was flooding near the San Antonio River Walk last night, and debris is blocking part of the trail.\n",
    "\n",
    "```\n",
    "\n",
    "**Model Output:**\n",
    "```\n",
    "\n",
    "Public Works\n",
    "\n",
    "```\n",
    "\n",
    "The model correctly identifies the topic without needing prior examples.  \n",
    "Zero-shot prompting is ideal for straightforward, well-defined tasks where the model can rely on general world knowledge.\n",
    "\n",
    "### Limitations\n",
    "Zero-shot prompting becomes less reliable when:\n",
    "- The task has an unusual format or label space.\n",
    "- The instructions are ambiguous.\n",
    "- The problem requires reasoning across multiple facts or steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6569634a-c4c4-4a2c-ac7c-f0f603f6b2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from utils import accuracy, ner_accuracy\n",
    "\n",
    "# Point this at your vLLM (OpenAI‑compatible) endpoint\n",
    "client = OpenAI(\n",
    "    base_url=\"http://10.246.100.142:8000/v1\",\n",
    "    api_key=\"token-abc123\"\n",
    ")\n",
    "\n",
    "# Small helper for convenience\n",
    "def call_model(messages, model=\"meta-llama/Llama-3.1-70B-Instruct\", max_tokens=512, temperature=0.0):\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        messages=messages\n",
    "    )\n",
    "    return resp.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2488ae09-f248-4a4f-b48a-d96a3a65d177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd classify that message as Public Works.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a kind, encouraging assistant who keeps answers brief. Classify the following message as one of: Public Works, Community Events, or Public Safety.\"},\n",
    "    {\"role\": \"user\", \"content\": \"There was flooding near the San Antonio River Walk last night, and debris is blocking part of the trail.\"}\n",
    "\n",
    "]\n",
    "\n",
    "print(call_model(messages))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f846038-43ec-421d-817f-4d98cc599c39",
   "metadata": {},
   "source": [
    "## 2. Few-Shot Prompting\n",
    "\n",
    "Few-shot prompting teaches a model by including a few examples of the desired behavior directly in the prompt.  \n",
    "Each example contains both an input and an output, allowing the model to learn the intended pattern through **in-context learning**.\n",
    "\n",
    "### Example: Classifying Community Messages with Examples\n",
    "\n",
    "Suppose the City of San Antonio wants to automatically route resident messages to the appropriate department.\n",
    "\n",
    "**Prompt:**\n",
    "```\n",
    "\n",
    "Message: The trash pickup on my street was missed this week.\n",
    "Category: Public Works\n",
    "\n",
    "Message: Where can I find information about this year’s Fiesta events?\n",
    "Category: Community Events\n",
    "\n",
    "Message: The streetlight on my block has been flickering for several nights.\n",
    "Category:\n",
    "\n",
    "```\n",
    "\n",
    "**Model Output:**\n",
    "```\n",
    "\n",
    "Public Works\n",
    "\n",
    "```\n",
    "\n",
    "By including a few examples, the model infers how to perform the classification task and outputs the correct category.\n",
    "\n",
    "Research (Brown et al., 2020; Min et al., 2022) shows that:\n",
    "- The format and consistency of examples matter more than the exact content.\n",
    "- Even a small number of demonstrations (e.g., three to five) can greatly improve results.\n",
    "- Few-shot prompting works best for structured or repetitive tasks.\n",
    "\n",
    "### Limitations\n",
    "Few-shot prompting may still fail on tasks requiring deeper reasoning or multi-step analysis.  \n",
    "In those cases, we can use **chain-of-thought prompting** to guide the model’s reasoning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b4342df-ad17-41fb-97ae-76c26c1ebdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public Works\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a kind, encouraging assistant who keeps answers brief. Classify the following message as one of: Public Works, Community Events, or Public Safety.\"},\n",
    "    {\"role\": \"user\", \"content\": \"The trash pickup on my street was missed this week.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Public Works\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where can I find information about this year’s Fiesta events?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Community Events\"},\n",
    "    {\"role\": \"user\", \"content\": \"There was flooding near the San Antonio River Walk last night, and debris is blocking part of the trail.\"}\n",
    "\n",
    "]\n",
    "\n",
    "print(call_model(messages))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ff37ee-13f4-48a6-83b9-368a8cb44d14",
   "metadata": {},
   "source": [
    "## 3. Chain-of-Thought Prompting\n",
    "\n",
    "Chain-of-thought (CoT) prompting (Wei et al., 2022) asks the model to think through the problem step by step before answering.\n",
    "It works well for planning, decision making, and contextual reasoning where multiple clues must be combined.\n",
    "\n",
    "### Example: Classifying a Civic Report for Public Works\n",
    "\n",
    "You are routing resident reports to the right city team. Ask the model to reason first, then give the label on a new line.\n",
    "\n",
    "**Prompt:**\n",
    "\n",
    "```\n",
    "You are a kind, encouraging assistant who keeps answers brief.\n",
    "Classify the following message as one of: Public Works, Community Events, or Public Safety.\n",
    "Reason step-by-step before returning the answer on a new line.\n",
    "\n",
    "Message: \"There was flooding near the San Antonio River Walk last night, and debris is blocking part of the trail.\"\n",
    "A:\n",
    "```\n",
    "\n",
    "**Model Output (with chain-of-thought reasoning):**\n",
    "\n",
    "```\n",
    "The report concerns flooding impacts on city infrastructure (trail) and debris removal.\n",
    "This aligns with maintenance/repair and right-of-way cleanup handled by public works.\n",
    "No immediate criminal activity or police/fire response indicated; not an event announcement.\n",
    "\n",
    "Public Works\n",
    "```\n",
    "\n",
    "The model connects the clues (infrastructure, debris removal, non-emergency) and then gives the final label on a separate line. This mirrors how a dispatcher or 311 triage system reasons before routing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4383ab35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To classify the message, let's break it down:\n",
      "\n",
      "1. The message mentions a specific location, the San Antonio River Walk, but this doesn't necessarily point to a specific category.\n",
      "2. The message reports an incident, which suggests it might be related to safety or a community issue.\n",
      "3. The incident is a robbery, which implies a crime occurred.\n",
      "\n",
      "Based on these steps, the message is most closely related to safety and crime.\n",
      "\n",
      " \n",
      "Public Safety\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a kind, encouraging assistant who keeps answers brief. Classify the following message as one of: Public Works, Community Events, or Public Safety. Reason step-by-step before returning the answer on a new line.\"},\n",
    "    {\"role\": \"user\", \"content\": \"There was robbery near the San Antonio River Walk last night.\"}\n",
    "\n",
    "]\n",
    "\n",
    "print(call_model(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a94c09-7b2b-4018-afce-12ed536fc162",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Summary\n",
    "\n",
    "| Technique | Description | Best For | Example Cue |\n",
    "|------------|--------------|-----------|--------------|\n",
    "| **Zero-Shot Prompting** | Uses no examples to teach the model a task or format | Classification, writing style, structured output | “Message: ... Category: ...” with several examples |\n",
    "| **Few-Shot Prompting** | Uses examples to teach the model a task or format | Classification, writing style, structured output | “Message: ... Category: ...” with several examples |\n",
    "| **Chain-of-Thought Prompting** | Encourages explicit reasoning before answering | Planning, decision-making, multi-factor analysis | “Let’s think step by step.” |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d59610",
   "metadata": {},
   "source": [
    "# Exercise 1: Classifying Public Opinions on Project Marvel\n",
    "\n",
    "## Background\n",
    "**Project Marvel** is a major redevelopment proposal in San Antonio centered around a new Spurs arena and entertainment district downtown. The plan involves both public and private funding and has sparked intense community discussion. Supporters emphasize economic growth, tourism, and long-term city identity. Critics highlight cost, fairness, and the city’s past experiences with similar promises.\n",
    "\n",
    "To study how large language models understand local political sentiment, we’ll use real comments collected from **Reddit (r/SanAntonio)** about Project Marvel. Each comment expresses a stance either **for (pro)** or **against** the project.\n",
    "\n",
    "\n",
    "## Objective\n",
    "Your task is to design a prompt that most accurately classifies each comment as **pro** or **against** Project Marvel.\n",
    "\n",
    "You’ll start simple and progressively improve your results using:\n",
    "1. **Prompt detail**: refine how the task is described.\n",
    "2. **Few-shot prompting**: add a few examples to guide the model.\n",
    "3. **Chain-of-thought reasoning**: let the model reason privately before deciding.\n",
    "\n",
    "**TASK**: Experiment to get the highest accuracy you can acheive!\n",
    "\n",
    "**IMPORTANT**: You must ensure that the word either 1) appears alone with nothing else; or 2) appears alone an a new line at the very end of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a544d84-950f-454c-a6fe-042ddaf06b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'comment': 'My question is when have SSE done ANYTHING in the past to screw '\n",
      "             'this city? Why all of this distrust for a team that has '\n",
      "             'ingrained itself into the identity of this community?',\n",
      "  'label': 'pro'},\n",
      " {'comment': 'The opposition to these props is not coming from blind distrust. '\n",
      "             'It could be argued that the PR campaign paid for by SSE is '\n",
      "             'dishonest for a few reasons.  Big arenas / stadiums like this '\n",
      "             'never generate meaningful revitalization or net gain in economic '\n",
      "             'activity, yet that’s one of the main selling points... ‘Good '\n",
      "             'jobs’ is a dubious claim. What exactly are the net new jobs from '\n",
      "             'this project? The Spurs and SSE are already here, no new jobs '\n",
      "             'there. So, likely just the temporary construction jobs.',\n",
      "  'label': 'against'},\n",
      " {'comment': 'The property tax revenue can’t go up - show me how that can '\n",
      "             'happen, we are committing the TIRZ to the arena indefinitely.  '\n",
      "             'We funded that in 2017, we are giving the revenue gains since '\n",
      "             'then away - we are losing property tax revenue, not gaining it.',\n",
      "  'label': 'against'},\n",
      " {'comment': 'See my comment above yours with a little history. This is the '\n",
      "             '4th arena for the Spurs with the same “economic boom” promises '\n",
      "             'that never delivered.  The 4th arena project can be on their '\n",
      "             'dime this time.',\n",
      "  'label': 'against'},\n",
      " {'comment': 'Because they can afford to pay for their own stadium like the '\n",
      "             \"Broncos owners did. It's billionaires from AirBNB and Dell that \"\n",
      "             \"own the team. Vote AGAINST A & B so our city doesn't give free \"\n",
      "             'rides to billionaires.',\n",
      "  'label': 'against'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pprint\n",
    "#from sklearn.metrics import f1_score\n",
    "\n",
    "with open('data/project_marvel.json') as iFile:\n",
    "    data = json.load(iFile)\n",
    "pprint.pprint(data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8c239ea0-d0e8-4e88-80c2-e9b2246ddee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL WITH NEW PROMPTS\n",
    "messages_cla = [\n",
    "    {\"role\": \"system\", \"content\": \"Only return either pro or against.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c795f4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.625\n"
     ]
    }
   ],
   "source": [
    "# NOTHING TO CHANGE HERE\n",
    "preds_cla = []\n",
    "true_cla = []\n",
    "for example in data:\n",
    "    true_cla.append(example['label'])\n",
    "    response = call_model(messages_cla+[{\"role\": \"user\", \"content\": example['comment']}], temperature=0.0, max_tokens=1000)\n",
    "    preds_cla.append(response)\n",
    "\n",
    "acc = accuracy(true_cla, preds_cla) # This cleans the preds before calcluating the accuracy\n",
    "print(f\"Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "659fed61-462a-43ef-826f-a3720f9c53d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1:\n",
      "{'label': 'pro', 'comment': 'My question is when have SSE done ANYTHING in the past to screw this city? Why all of this distrust for a team that has ingrained itself into the identity of this community?'}\n",
      "\n",
      "Against\n",
      "\n",
      "Response 2:\n",
      "{'label': 'against', 'comment': 'The opposition to these props is not coming from blind distrust. It could be argued that the PR campaign paid for by SSE is dishonest for a few reasons.  Big arenas / stadiums like this never generate meaningful revitalization or net gain in economic activity, yet that’s one of the main selling points... ‘Good jobs’ is a dubious claim. What exactly are the net new jobs from this project? The Spurs and SSE are already here, no new jobs there. So, likely just the temporary construction jobs.'}\n",
      "\n",
      "Against\n",
      "\n",
      "Response 3:\n",
      "{'label': 'against', 'comment': 'The property tax revenue can’t go up - show me how that can happen, we are committing the TIRZ to the arena indefinitely.  We funded that in 2017, we are giving the revenue gains since then away - we are losing property tax revenue, not gaining it.'}\n",
      "\n",
      "Against\n"
     ]
    }
   ],
   "source": [
    "print(\"Response 1:\")\n",
    "print(data[0])\n",
    "print()\n",
    "print(preds_cla[0])\n",
    "print()\n",
    "print(\"Response 2:\")\n",
    "print(data[1])\n",
    "print()\n",
    "print(preds_cla[1])\n",
    "print()\n",
    "print(\"Response 3:\")\n",
    "print(data[2])\n",
    "print()\n",
    "print(preds_cla[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d7d9e-1567-4ed7-be83-b5abd4426917",
   "metadata": {},
   "source": [
    "# Exercise 2: Named Entity Recognition (NER) for San Antonio Restaurant Names\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this exercise, you’ll build prompts to extract **restaurant names** from short sentences gathered from **r/SanAntonio**. The goal is to identify restaurant entities accurately and consistently using prompt design.\n",
    "\n",
    "### Task\n",
    "\n",
    "Given a sentence, return a list of spans for **restaurant names** (proper names of eateries, food trucks, cafes, bakeries, bars that serve food). Ignore cuisine types, general food words, and locations unless they are part of the restaurant’s name.\n",
    "\n",
    "### Your Goal\n",
    "\n",
    "Maximize extraction quality (precision and recall) on held-out examples by improving your prompts through:\n",
    "\n",
    "1. **Prompt details** (clear instructions, definitions, counter-examples)\n",
    "2. **Few-shot prompting** (a handful of labeled examples)\n",
    "3. **Chain-of-thought reasoning** (brief reasoning steps that justify which spans are names vs. not)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f80bcdbf-49f3-4951-a708-5c3fd687c8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entities': [{'label': 'RESTAURANT', 'name': 'comfort cafe'}],\n",
      "  'text': 'I love comfort cafe off Starcrest in that little shopping center '\n",
      "          'area I forgot the name of the center.'},\n",
      " {'entities': [{'label': 'RESTAURANT', 'name': 'Comfort Cafe'}],\n",
      "  'text': 'Comfort Cafe was really good.'},\n",
      " {'entities': [{'label': 'RESTAURANT', 'name': 'Mi Tierra'}],\n",
      "  'text': 'Mi Tierra for Breakfast? A lot of people say that place sucks '\n",
      "          'altogether.'},\n",
      " {'entities': [{'label': 'RESTAURANT', 'name': 'Garcia’s'}],\n",
      "  'text': 'ETA: Garcia’s for their brisket taco.'},\n",
      " {'entities': [{'label': 'RESTAURANT', 'name': 'Blanco Cafe'}],\n",
      "  'text': 'Blanco Cafe (the one on Blanco) has great enchiladas too.'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pprint\n",
    "\n",
    "with open('data/restaurants.json') as iFile:\n",
    "    ner_data = json.load(iFile)\n",
    "pprint.pprint(ner_data[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2afeb42-9f24-4cfd-bddc-702f14db812c",
   "metadata": {},
   "source": [
    "**IMPORTANT**: Your prompt must have all entities listed as a list as the last line in the response with **nothing** else on the line. Multiple entites must be seperated by a comma. \n",
    "\n",
    "Give the following input:\n",
    "\n",
    "``I love the food at McDonalds and Comfort Cafe.''\n",
    "\n",
    "Here are valid and invalid outputs\n",
    "\n",
    "**VALID OUTPUTS**:\n",
    "\n",
    "```\n",
    "McDonalds,Comfort Cafe\n",
    "```\n",
    "\n",
    "```\n",
    "The author is talking about food at two restaurants:\n",
    "McDonalds,Comfort Cafe\n",
    "```\n",
    "\n",
    "**INVALID OUTPUTS**:\n",
    "\n",
    "```\n",
    "The answer is: McDonalds,Comfort Cafe\n",
    "```\n",
    "\n",
    "```\n",
    "The author is talking about food at two restaurants: McDonalds,Comfort Cafe\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0e71904d-3ec8-4ecc-8d06-54d724fd7fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL WITH NEW PROMPTS\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Extract all names comma-seperated list return nothing else.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "44e0ba29-7844-4d54-82e5-c2ce8372a925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Royal Inn\n",
      "Accuracy: 0.3111111111111111\n"
     ]
    }
   ],
   "source": [
    "# NOTHING TO CHANGE HERE\n",
    "preds = []\n",
    "true = []\n",
    "for example in ner_data:\n",
    "    true.append([label['name'].lower() for label in example['entities']])\n",
    "    response = call_model(messages+[{\"role\": \"user\", \"content\": example['text']}], temperature=0.0, max_tokens=1000)\n",
    "    preds.append(response)\n",
    "print(preds[-1])\n",
    "\n",
    "acc = ner_accuracy(true, preds) # This cleans the preds before calcluating the accuracy\n",
    "print(f\"Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c2790b0c-37ee-4611-86c7-a8853efd83d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1:\n",
      "{'text': 'I love comfort cafe off Starcrest in that little shopping center area I forgot the name of the center.', 'entities': [{'name': 'comfort cafe', 'label': 'RESTAURANT'}]}\n",
      "\n",
      "No names were mentioned.\n",
      "\n",
      "Response 2:\n",
      "{'text': 'Comfort Cafe was really good.', 'entities': [{'name': 'Comfort Cafe', 'label': 'RESTAURANT'}]}\n",
      "\n",
      "Comfort Cafe\n",
      "\n",
      "Response 3:\n",
      "{'text': 'Mi Tierra for Breakfast? A lot of people say that place sucks altogether.', 'entities': [{'name': 'Mi Tierra', 'label': 'RESTAURANT'}]}\n",
      "\n",
      "Mi Tierra\n"
     ]
    }
   ],
   "source": [
    "print(\"Response 1:\")\n",
    "print(ner_data[0])\n",
    "print()\n",
    "print(preds[0])\n",
    "print()\n",
    "print(\"Response 2:\")\n",
    "print(ner_data[1])\n",
    "print()\n",
    "print(preds[1])\n",
    "print()\n",
    "print(\"Response 3:\")\n",
    "print(ner_data[2])\n",
    "print()\n",
    "print(preds[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82837200-b70b-4a84-af7b-9c3c40623387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
